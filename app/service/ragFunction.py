import pandas as pd
import shutil
import os
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv
from app.utils.s3_utils import read_all_csv_from_s3  # ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìœ ì§€

load_dotenv()

SUBJECT_MAP = {
    "ê²½ì œ": "economics",
    "êµ­ì–´": "korean",
    "ë™ì•„ì‹œì•„ì‚¬": "east_asian_history",
    "ë¬¼ë¦¬1": "physics1",
    "ë¬¼ë¦¬2": "physics2",
    "ì‚¬íšŒë¬¸í™”": "sociology",
    "ìƒëª…ê³¼í•™1": "biology1",
    "ìƒëª…ê³¼í•™2": "biology2",
    "ìƒí™œê³¼ ìœ¤ë¦¬": "ethics_life",
    "ì„¸ê³„ì‚¬": "world_history",
    "ì„¸ê³„ì§€ë¦¬": "world_geography",
    "ìˆ˜í•™": "math",
    "ì˜ì–´": "english",
    "ìœ¤ë¦¬ì™€ ì‚¬ìƒ": "ethics_philosophy",
    "ì§€êµ¬ê³¼í•™1": "earth_science1",
    "ì§€êµ¬ê³¼í•™2": "earth_science2",
    "í†µí•©ì‚¬íšŒ1": "integrated_social1",
    "í†µí•©ì‚¬íšŒ1,2": "integrated_social1_2",
    "í†µí•©ì‚¬íšŒ2": "integrated_social2",
    "í•œêµ­ì‚¬": "korean_history",
    "í•œêµ­ì§€ë¦¬": "korean_geography",
    "í™”í•™1": "chemistry1",
    "í™”í•™2": "chemistry2"
}

def create_all_subject_vector_db(platform: str):
    try:
        results = []

        for subject in SUBJECT_MAP.keys():
            subject_slug = SUBJECT_MAP[subject]
            print(f"ğŸ”„ {platform} / {subject} ë²¡í„° DB ìƒì„± ì¤‘...")

            db_dir = os.path.abspath(f"./chroma_db/{platform}_{subject_slug}")
            if os.path.exists(db_dir):
                print(f"ê¸°ì¡´ DB ë””ë ‰í† ë¦¬ ì‚­ì œ: {db_dir}")
                shutil.rmtree(db_dir)

            # S3ì—ì„œ platformê³¼ subjectì— ë§ëŠ” CSVë§Œ ì½ê¸°
            csv_files = read_all_csv_from_s3(platform=platform, subject=subject)
            if not csv_files:
                print(f"âš ï¸ {platform} / {subject}: S3 ë°ì´í„° ì—†ìŒ")
                results.append(f"{platform} / {subject}: ë°ì´í„° ì—†ìŒ")
                continue

            courses_dfs, lectures_dfs = [], []

            for file_name, file_content in csv_files:
                if "courses" in file_name:
                    courses_dfs.append(file_content)
                elif "lectures" in file_name:
                    lectures_dfs.append(file_content)

            if not courses_dfs or not lectures_dfs:
                print(f"âš ï¸ {platform} / {subject}: ë°ì´í„° ë¶ˆì™„ì „")
                results.append(f"{platform} / {subject}: ë°ì´í„° ë¶ˆì™„ì „")
                continue

            courses_df = pd.concat(courses_dfs, ignore_index=True)
            lectures_df = pd.concat(lectures_dfs, ignore_index=True)

            texts, metadatas = [], []

            for _, course_row in courses_df.iterrows():
                course_id = course_row['course_id']
                course_title = course_row['title']
                teacher = course_row['teacher']
                description = course_row['description']
                subject_name = course_row['subject']
                grade = course_row['grade']
                platform_name = course_row['platform']
                is_paid = course_row['is_paid']
                price = course_row['price']
                url = course_row['url']

                related_lectures = lectures_df[lectures_df['course_id'] == course_id]
                lecture_texts = [
                    f"- {lecture_row['title']} / {lecture_row['info']}"
                    for _, lecture_row in related_lectures.iterrows()
                ]
                lectures_combined = "\n".join(lecture_texts) if lecture_texts else "ê°•ì˜ ì—†ìŒ"

                full_text = f"""
                    ê°•ì˜ ID: {course_id}
                    ê³¼ëª©: {subject_name}
                    í•™ë…„: {grade}
                    ê°•ì˜ëª…: {course_title}
                    ê°•ì‚¬: {teacher}
                    ì„¤ëª…: {description}
                    í”Œë«í¼: {platform_name}
                    ê°€ê²©: {price}
                    url: {url}
                    ìœ ë£Œ/ë¬´ë£Œ: {"ìœ ë£Œ" if is_paid else "ë¬´ë£Œ"}
                    ê°•ì˜ ë¦¬ìŠ¤íŠ¸:
                    {lectures_combined}
                    """
                texts.append(full_text)
                metadatas.append({"source": f"{platform}_{subject_slug}", "course_id": course_id})

            embeddings = OpenAIEmbeddings()
            db = Chroma(
                embedding_function=embeddings,
                persist_directory=db_dir,
                collection_name=f"{platform}_{subject_slug}_collection"
            )

            # ë°°ì¹˜ë¡œ ì„ë² ë”© ì¶”ê°€
            batch_size = 100
            for i in range(0, len(texts), batch_size):
                db.add_texts(
                    texts=texts[i:i + batch_size],
                    metadatas=metadatas[i:i + batch_size]
                )

            db.persist()
            print(f"âœ… {platform} / {subject} â†’ ì´ {len(texts)}ê°œ ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ")
            results.append(f"{platform} / {subject}: {len(texts)}ê°œ ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ")

        return results

    except Exception as e:
        return f"Error occurred: {str(e)}"
